{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer model for language translation\n",
    "\n",
    "In this notebook, I am implementing from scratch the Transformer, a network architecture that was proposed in the paper \"Attention Is All You Need\" (Vaswani et al., Attention is all you need, Advances in neural information processing systems, 2017) and I am applying it to translate sentences from Portuguese to English. The Transformer is based solely on attention mechanisms, without any use of LSTMs or RNNs.\n",
    "\n",
    "The dataset, the hyperparameters' values and the examples used for the evaluation were taken from https://www.tensorflow.org/tutorials/text/transformer for comparison reasons. The figures are taken from the above referenced paper.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n",
      "Eager execution: True\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "tf.executing_eagerly()\n",
    "\n",
    "print(tf.__version__)\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use of Portuguese-English translation dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following cells which load and process the dataset were taken from https://www.tensorflow.org/tutorials/text/transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
    "                               as_supervised=True)\n",
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (en.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n",
    "\n",
    "tokenizer_pt = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (pt.numpy() for pt, en in train_examples), target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def encode(lang1, lang2):\n",
    "  lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(\n",
    "      lang1.numpy()) + [tokenizer_pt.vocab_size+1]\n",
    "\n",
    "  lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n",
    "      lang2.numpy()) + [tokenizer_en.vocab_size+1]\n",
    "  \n",
    "  return lang1, lang2\n",
    "\n",
    "def tf_encode(pt, en):\n",
    "  result_pt, result_en = tf.py_function(encode, [pt, en], [tf.int64, tf.int64])\n",
    "  result_pt.set_shape([None])\n",
    "  result_en.set_shape([None])\n",
    "\n",
    "  return result_pt, result_en\n",
    "\n",
    "MAX_LENGTH = 40\n",
    "\n",
    "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
    "    return tf.logical_and(tf.size(x) <= max_length,\n",
    "                        tf.size(y) <= max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preprocessed = (\n",
    "    train_examples\n",
    "    .map(tf_encode) \n",
    "    .filter(filter_max_length)\n",
    "    # cache the dataset to memory to get a speedup while reading from it.\n",
    "    .cache()\n",
    "    .shuffle(BUFFER_SIZE))\n",
    "\n",
    "val_preprocessed = (\n",
    "    val_examples\n",
    "    .map(tf_encode)\n",
    "    .filter(filter_max_length))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (train_preprocessed\n",
    "                 .padded_batch(BATCH_SIZE, padded_shapes=([None], [None]))\n",
    "                 .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "val_dataset = (val_preprocessed\n",
    "               .padded_batch(BATCH_SIZE,  padded_shapes=([None], [None])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8214\n",
      "8087\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_pt.vocab_size)\n",
    "print(tokenizer_en.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(tokenizer_en.vocab_size):\n",
    "#     print(i,tokenizer_en.decode([i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(tokenizer_pt.vocab_size):\n",
    "#     print(i,tokenizer_pt.decode([i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the training dataset in proper format for TensorFlow 1.15\n",
    "X_train = []\n",
    "Y_train = []\n",
    "for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "    X_train.append(inp.numpy())\n",
    "    Y_train.append(tar.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create input placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inputs():\n",
    "    \n",
    "    inputs_ = tf.placeholder(tf.int32,[None,None],name='inputs_')\n",
    "    targets_inputs_ = tf.placeholder(tf.int32,[None,None],name='targets_inputs_')\n",
    "    targets_real_ = tf.placeholder(tf.int32,[None,None],name='targets_')\n",
    "    \n",
    "    return inputs_, targets_inputs_, targets_real_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled dot product attention\n",
    "\n",
    "<img src=\"images/scaled_dot_product_attention.png\" style=\"width:300;height:300px;\"> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    " \n",
    "    # matmul\n",
    "    q_kTranspose = tf.matmul(q, tf.transpose(k, [0, 2, 1]))  # (batch_size, Tq, Tk)\n",
    "  \n",
    "    # scale \n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    logits = q_kTranspose / tf.math.sqrt(dk)\n",
    "\n",
    "    # mask\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)  \n",
    "\n",
    "    # softmax \n",
    "    weights = tf.nn.softmax(logits, axis=-1)  # (batch_size, Tq, Tk)\n",
    "\n",
    "    # matmul\n",
    "    output = tf.matmul(weights, v)  # (batch_size, Tq, dv)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "<img src=\"images/mha.png\" style=\"width:300;height:300px;\"> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(v, k, q, mask, d_model, num_heads, scope):\n",
    "    \n",
    "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
    "    \n",
    "        q = tf.contrib.layers.fully_connected(q, d_model, activation_fn = None,\n",
    "                            weights_initializer = tf.keras.initializers.glorot_uniform()) \n",
    "        # (batch_size, Tq, d_model)\n",
    "        k = tf.contrib.layers.fully_connected(k, d_model, activation_fn = None,\n",
    "                            weights_initializer = tf.keras.initializers.glorot_uniform())\n",
    "        # (batch_size, Tk, d_model)\n",
    "        v = tf.contrib.layers.fully_connected(v, d_model, activation_fn = None,\n",
    "                            weights_initializer = tf.keras.initializers.glorot_uniform())\n",
    "        # (batch_size, Tv, d_model)\n",
    "            \n",
    "        q = tf.split(q, num_heads, axis=2) \n",
    "        k = tf.split(k, num_heads, axis=2) \n",
    "        v = tf.split(v, num_heads, axis=2) \n",
    "        # lists of num_heads elements with each of them having size (batch_size, T, d_model/num_heads) \n",
    "        # where T is Tq or Tk or Tv\n",
    "        \n",
    "        attention = []\n",
    "        for head_i in range(num_heads):\n",
    "            attention_i = scaled_dot_product_attention(q[head_i], k[head_i], v[head_i], mask)\n",
    "            # batch_size, Tq, d_model/num_heads)\n",
    "            attention.append(attention_i)\n",
    "        \n",
    "        concat_attention = tf.concat(attention, axis = 2) # (batch_size, Tq, d_model)\n",
    "        \n",
    "        output = tf.contrib.layers.fully_connected(concat_attention, d_model, activation_fn = None,\n",
    "                            weights_initializer = tf.keras.initializers.glorot_uniform())\n",
    "        # (batch_size, Tq, d_model)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position wise feed forward network\n",
    "\n",
    "<img src=\"images/pwffn.png\" style=\"width:50;height:50px;\"> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(input_, d_model, d_ff, scope):\n",
    "\n",
    "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
    "        x1 = tf.contrib.layers.fully_connected(input_, d_ff, activation_fn = tf.nn.relu,\n",
    "                            weights_initializer = tf.keras.initializers.glorot_uniform())  \n",
    "        # (batch_size, T, d_ff)\n",
    "        ffn = tf.contrib.layers.fully_connected(x1, d_model, activation_fn = None,\n",
    "                            weights_initializer = tf.keras.initializers.glorot_uniform()) \n",
    "        # (batch_size, T, d_model)\n",
    "            \n",
    "    return ffn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder layer\n",
    "\n",
    "\n",
    "<img src=\"images/encoder_layer.png\" style=\"width:400;height:400px;\"> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(x, mask, d_model, d_ff, num_heads, rate, scope):\n",
    "\n",
    "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
    "        attention_output = multi_head_attention(x, x, x, mask, d_model, num_heads, 'mha')\n",
    "        attention_output = tf.nn.dropout(attention_output, rate = rate)\n",
    "        output_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x + attention_output)\n",
    "    \n",
    "        ffn_output = point_wise_feed_forward_network(output_1, d_model, d_ff, 'pwffn')\n",
    "        ffn_output = tf.nn.dropout(ffn_output, rate = rate)\n",
    "        output_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(output_1 + ffn_output) \n",
    "    \n",
    "    return output_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder layer\n",
    "\n",
    "\n",
    "<img src=\"images/decoder_layer.png\" style=\"width:500;height:500px;\"> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_layer(y, encoder_output, padding_mask, future_mask, d_model, d_ff, num_heads, rate, scope):\n",
    "\n",
    "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
    "        attention_output_1 = multi_head_attention(y, y, y, future_mask, \n",
    "                                                                    d_model, num_heads, 'mha_1')\n",
    "        attention_output_1 = tf.nn.dropout(attention_output_1, rate = rate)\n",
    "        output_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(y + attention_output_1)\n",
    "\n",
    "        attention_output_2 = multi_head_attention(encoder_output, encoder_output, output_1, \n",
    "                                                                    padding_mask, d_model, num_heads, 'mha_2')\n",
    "        attention_output_2 = tf.nn.dropout(attention_output_2, rate = rate)\n",
    "        output_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention_output_2 + output_1) \n",
    "    \n",
    "        ffn_output = point_wise_feed_forward_network(output_2, d_model, d_ff, 'pwffn')\n",
    "        ffn_output = tf.nn.dropout(ffn_output, rate = rate)\n",
    "        output_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(output_2 + ffn_output)\n",
    "    \n",
    "    return output_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x, mask, num_layers, d_model, num_heads, d_ff, input_vocab_size, maximum_position_encoding, rate, scope):\n",
    "\n",
    "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
    "        \n",
    "        T_x = tf.shape(x)[1]\n",
    "        pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        x = tf.keras.layers.Embedding(input_vocab_size, d_model)(x) # (batch_size, T_x, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "        x += pos_encoding[:, :T_x, :]\n",
    "\n",
    "        x = tf.nn.dropout(x, rate = rate)\n",
    "    \n",
    "        for i in range(num_layers):\n",
    "            x = encoder_layer(x, mask, d_model, d_ff, num_heads, rate, 'Encoder_layer_'+str(i+1))\n",
    "    \n",
    "    return x  # (batch_size, T_x, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(y, encoder_output, future_mask, padding_mask, num_layers, d_model, num_heads, d_ff, \n",
    "                        target_vocab_size, maximum_position_encoding, rate, scope):\n",
    "\n",
    "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):   \n",
    "        \n",
    "        T_y = tf.shape(y)[1] \n",
    "        attention_weights = {}    \n",
    "        pos_encoding = positional_encoding(maximum_position_encoding, d_model)    \n",
    "    \n",
    "        y = tf.keras.layers.Embedding(target_vocab_size, d_model)(y)  # (batch_size, T_y, d_model)\n",
    "        y *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "        y += pos_encoding[:, :T_y, :]\n",
    "    \n",
    "        y = tf.nn.dropout(y, rate = rate)\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            y = decoder_layer(y, encoder_output, padding_mask, future_mask, d_model, \n",
    "                                              d_ff, num_heads, rate, 'Decoder_layer_'+str(i+1))\n",
    "    \n",
    "    return y # (batch_size, T_y, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional encoding\n",
    "\n",
    "<img src=\"images/positional_encoding.png\" style=\"width:100;height:100px;\"> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "        \n",
    "    pos = np.arange(0,position)\n",
    "    pos = pos.reshape(-1,1)\n",
    "    \n",
    "    div_term = 10000**(np.arange(0, d_model, 2)/d_model)\n",
    "        \n",
    "    pe = np.zeros((position, d_model))\n",
    "    pe[:,0::2] = np.sin(pos/div_term)\n",
    "    pe[:,1::2] = np.cos(pos/div_term)\n",
    "        \n",
    "    positional_enc = tf.convert_to_tensor(pe, dtype=tf.float32) # (position, d_model)\n",
    "    positional_enc = positional_enc[np.newaxis, :,:] # (1, position, d_model)\n",
    "        \n",
    "    return positional_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(input_, target_, encoder_padding_mask, future_mask, decoder_padding_mask, num_layers, d_model, num_heads, d_ff,\n",
    "                   input_vocab_size, target_vocab_size, pe_input, pe_target, rate):\n",
    "    \n",
    "    \n",
    "    encoder_output = encoder(input_, encoder_padding_mask, num_layers, d_model, num_heads, d_ff, input_vocab_size, \n",
    "                                        pe_input, rate, 'Encoder')\n",
    "\n",
    "    decoder_output = decoder(target_, encoder_output, future_mask, decoder_padding_mask, \n",
    "                                          num_layers, d_model, num_heads, d_ff, target_vocab_size, pe_target, rate, 'Decoder')\n",
    "\n",
    "    final_output = tf.contrib.layers.fully_connected(decoder_output, target_vocab_size, activation_fn = None,\n",
    "                            weights_initializer = tf.keras.initializers.glorot_uniform())\n",
    "    \n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate\n",
    "\n",
    "\n",
    "<img src=\"images/lrate.png\" style=\"width:50;height:50px;\"> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_function(d_model, step_num):\n",
    "    \n",
    "    warmup_steps = 4000\n",
    "\n",
    "    step_num = tf.cast(step_num + 1,tf.float32)\n",
    "    d_model = tf.cast(d_model, tf.float32)\n",
    "    \n",
    "    part1 = tf.math.rsqrt(step_num)\n",
    "    part2 = step_num * (warmup_steps ** (-1.5))\n",
    "    \n",
    "    learning_rate = tf.math.rsqrt(d_model) * tf.math.minimum(part1, part2)\n",
    "    \n",
    "    return learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(target_, prediction_):\n",
    "  \n",
    "    mask = tf.math.logical_not(tf.math.equal(target_, 0))\n",
    "    \n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')(target_, prediction_)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "  \n",
    "    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_function(target_, prediction_):\n",
    "    \n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(target_,tf.cast(tf.argmax(prediction_, axis=-1),dtype=tf.int32)),dtype=tf.float32))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer_function(loss, d_model):\n",
    "    \n",
    "    global_step = tf.Variable(0, trainable=False) \n",
    "    learning_rate = learning_rate_function(d_model, global_step)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.98, epsilon=1e-09).minimize(loss, global_step)\n",
    "     \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_mask_function(input_):\n",
    "    \n",
    "    input_ = tf.cast(tf.math.equal(input_, 0), tf.float32) # (batch_size, T)\n",
    "  \n",
    "    # add extra dimension to add the padding to the attention logits\n",
    "    return input_[:, tf.newaxis, :]  # (batch_size, 1, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def future_mask_function(batch_size_, size):\n",
    "    \n",
    "    diag_vals = tf.ones((size, size))  # (T, T)\n",
    "    tril = 1 - tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense() # (T, T)\n",
    "    mask = tf.tile(tril[tf.newaxis, :, :], [batch_size_, 1, 1]) # (batch_size_, T, T)\n",
    "\n",
    "    return mask  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(input_, target_):\n",
    "    \n",
    "    # used in the input of the encoder to mask the padding tokens\n",
    "    encoder_padding_mask = padding_mask_function(input_)\n",
    "    \n",
    "    # Used in the 1st attention block of the decoder to mask the padding tokens of the decoder input \n",
    "    # and prevent positions from attending to subsequent positions of the decoder input \n",
    "    future_mask = future_mask_function(tf.shape(target_)[0], tf.shape(target_)[1])\n",
    "    decoder_target_padding_mask = padding_mask_function(target_)\n",
    "    future_pad_mask = tf.maximum(decoder_target_padding_mask, future_mask)\n",
    "  \n",
    "    # used in the 2nd attention block of the decoder to mask the padding tokens of the encoder outputs\n",
    "    decoder_padding_mask = padding_mask_function(input_)\n",
    "  \n",
    "    return encoder_padding_mask, future_pad_mask, decoder_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer_model:\n",
    "    \n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, input_vocab_size, target_vocab_size, pe_input, \n",
    "                       pe_target, rate):\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Build the input placeholder tensors\n",
    "        self.inputs_, self.targets_inputs_, self.targets_real_ = build_inputs()\n",
    "        \n",
    "        encoder_padding_mask, future_pad_mask, decoder_padding_mask = create_masks(self.inputs_, self.targets_inputs_)\n",
    "        \n",
    "        self.predictions = transformer(self.inputs_, self.targets_inputs_, encoder_padding_mask, future_pad_mask, \n",
    "                                     decoder_padding_mask,  num_layers, d_model, num_heads, d_ff, input_vocab_size, \n",
    "                                     target_vocab_size, pe_input, pe_target, rate)\n",
    "        \n",
    "        # Loss\n",
    "        self.loss = loss_function(self.targets_real_, self.predictions)\n",
    "        \n",
    "        # Accuracy\n",
    "        self.accuracy = accuracy_function(self.targets_real_, self.predictions)\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optimizer_function(self.loss, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "d_ff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = tokenizer_pt.vocab_size + 2\n",
    "target_vocab_size = tokenizer_en.vocab_size + 2\n",
    "dropout_rate = 0.1\n",
    "\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20 Mean loss  : 7.1167 Mean accuracy  : 0.0508\n",
      "Epoch: 2/20 Mean loss  : 5.0024 Mean accuracy  : 0.1258\n",
      "Epoch: 3/20 Mean loss  : 4.4511 Mean accuracy  : 0.1512\n",
      "Epoch: 4/20 Mean loss  : 4.0000 Mean accuracy  : 0.1752\n",
      "Epoch: 5/20 Mean loss  : 3.5342 Mean accuracy  : 0.2021\n",
      "Epoch: 6/20 Mean loss  : 3.1338 Mean accuracy  : 0.2232\n",
      "Epoch: 7/20 Mean loss  : 2.7316 Mean accuracy  : 0.2454\n",
      "Epoch: 8/20 Mean loss  : 2.4082 Mean accuracy  : 0.2640\n",
      "Epoch: 9/20 Mean loss  : 2.1739 Mean accuracy  : 0.2779\n",
      "Epoch: 10/20 Mean loss  : 1.9993 Mean accuracy  : 0.2883\n",
      "Epoch: 11/20 Mean loss  : 1.8562 Mean accuracy  : 0.2975\n",
      "Epoch: 12/20 Mean loss  : 1.7389 Mean accuracy  : 0.3052\n",
      "Epoch: 13/20 Mean loss  : 1.6423 Mean accuracy  : 0.3111\n",
      "Epoch: 14/20 Mean loss  : 1.5561 Mean accuracy  : 0.3170\n",
      "Epoch: 15/20 Mean loss  : 1.4852 Mean accuracy  : 0.3218\n",
      "Epoch: 16/20 Mean loss  : 1.4178 Mean accuracy  : 0.3263\n",
      "Epoch: 17/20 Mean loss  : 1.3592 Mean accuracy  : 0.3307\n",
      "Epoch: 18/20 Mean loss  : 1.3034 Mean accuracy  : 0.3349\n",
      "Epoch: 19/20 Mean loss  : 1.2543 Mean accuracy  : 0.3382\n",
      "Epoch: 20/20 Mean loss  : 1.2102 Mean accuracy  : 0.3415\n",
      "Time taken for ALL epochs: 1347.9070222377777 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.disable_eager_execution()\n",
    "tf.reset_default_graph()\n",
    "\n",
    "model = transformer_model(num_layers = num_layers, d_model = d_model, num_heads = num_heads, d_ff = d_ff, \n",
    "                          input_vocab_size = input_vocab_size, target_vocab_size = target_vocab_size, \n",
    "                          pe_input = input_vocab_size, pe_target = target_vocab_size, rate = dropout_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=num_epochs) \n",
    "checkpoint_folder = 'checkpoints_transformer'\n",
    "\n",
    "start_total = time.time()\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "    train_loss_per_step = []\n",
    "    train_loss_per_epoch = []\n",
    "    train_accuracy_per_epoch = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "        \n",
    "        train_loss_per_batch = []\n",
    "        train_accuracy_per_batch = []\n",
    "        \n",
    "        for batch in range(len(X_train)):\n",
    "            \n",
    "            inputs_ = X_train[batch]\n",
    "            targets_inputs_ = Y_train[batch][:, :-1]\n",
    "            targets_real_ = Y_train[batch][:, 1:]\n",
    "\n",
    "            feed = {model.inputs_: inputs_,\n",
    "                   model.targets_inputs_: targets_inputs_,\n",
    "                   model.targets_real_: targets_real_}\n",
    "\n",
    "    \n",
    "            batch_predictions, batch_loss, batch_accuracy, _ = sess.run([model.predictions,\n",
    "                                                 model.loss,\n",
    "                                                 model.accuracy,\n",
    "                                                 model.optimizer],\n",
    "                                                 feed_dict=feed)\n",
    "        \n",
    "            \n",
    "            train_loss_per_batch.append(batch_loss)\n",
    "            train_loss_per_step.append(batch_loss)\n",
    "            train_accuracy_per_batch.append(batch_accuracy)\n",
    "            \n",
    "        train_loss_per_epoch.append(np.mean(train_loss_per_batch))\n",
    "        train_accuracy_per_epoch.append(np.mean(train_accuracy_per_batch))\n",
    "        \n",
    "        print(\"Epoch: {}/{}\".format(epoch+1, num_epochs),\n",
    "              \"Mean loss  : {:.4f}\".format(np.mean(train_loss_per_batch)),\n",
    "              \"Mean accuracy  : {:.4f}\".format(np.mean(train_accuracy_per_batch)))\n",
    "        \n",
    "    saver.save(sess, checkpoint_folder+'/transformer.ckpt')\n",
    "    print ('Time taken for ALL epochs: {} secs\\n'.format(time.time() - start_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figures of loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(np.arange(1,num_epochs+1), train_loss_per_epoch)\n",
    "plt.ylabel('mean loss per epoch')\n",
    "plt.xlabel('epoch')\n",
    "fig.savefig('./'+ checkpoint_folder +'/mean loss per epoch.png')\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(np.arange(1,num_epochs+1), train_accuracy_per_epoch)\n",
    "plt.ylabel('mean accuracy per epoch')\n",
    "plt.xlabel('epoch')\n",
    "fig.savefig('./'+ checkpoint_folder +'/mean accuracy per epoch.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The funtions 'evaluate' and 'translate' and the testing examples were taken from https://www.tensorflow.org/tutorials/text/transformer which uses TensorFlow 2.0 and were modified accordingly for TensorFlow 1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "\n",
    "    dropout_rate = 0\n",
    "    start_token = [tokenizer_pt.vocab_size]\n",
    "    end_token = [tokenizer_pt.vocab_size + 1]\n",
    "    \n",
    "    inp_sentence = start_token + tokenizer_pt.encode(inp_sentence) + end_token\n",
    "    encoder_input = np.expand_dims(inp_sentence, 0)\n",
    "    \n",
    "    decoder_input = [tokenizer_en.vocab_size]\n",
    "    output = np.expand_dims(decoder_input, 0)\n",
    "\n",
    "    model = transformer_model(num_layers = num_layers, d_model = d_model, num_heads = num_heads, d_ff = d_ff, \n",
    "                          input_vocab_size = input_vocab_size, target_vocab_size = target_vocab_size, \n",
    "                          pe_input = input_vocab_size, pe_target = target_vocab_size, rate = dropout_rate)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(checkpoint_folder))\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            \n",
    "            feed = {model.inputs_: encoder_input,\n",
    "                   model.targets_inputs_: output,\n",
    "                   model.targets_real_: output}\n",
    "            \n",
    "            batch_predictions = sess.run(model.predictions, feed_dict=feed)\n",
    "                    \n",
    "            # select the last word from the Ty dimension\n",
    "            predictions = batch_predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "            \n",
    "            predicted_id = np.argmax(predictions, axis=-1)\n",
    "            predicted_id = predicted_id.astype(int)\n",
    "            \n",
    "            # return the result if the predicted_id is equal to the end token\n",
    "            if predicted_id == tokenizer_en.vocab_size+1:\n",
    "                return np.squeeze(output, axis=0)\n",
    "        \n",
    "            # concatentate the predicted_id to the output which is given to the decoder # as its input.\n",
    "            output = np.concatenate((output, predicted_id), axis=-1)\n",
    "    \n",
    "        return np.squeeze(output, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    \n",
    "    result = evaluate(sentence)\n",
    "  \n",
    "    predicted_sentence = tokenizer_en.decode([i for i in result \n",
    "                                            if i < tokenizer_en.vocab_size])  \n",
    "\n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(predicted_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints_transformer/transformer.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints_transformer/transformer.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: este  um problema que temos que resolver.\n",
      "Predicted translation: this is a problem that we have to solve .\n",
      "Real translation: this is a problem we have to solve .\n"
     ]
    }
   ],
   "source": [
    "translate(\"este  um problema que temos que resolver.\")\n",
    "print (\"Real translation: this is a problem we have to solve .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints_transformer/transformer.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints_transformer/transformer.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: os meus vizinhos ouviram sobre esta ideia.\n",
      "Predicted translation: my neighbors heard about this idea .\n",
      "Real translation: and my neighboring homes heard about this idea .\n"
     ]
    }
   ],
   "source": [
    "translate(\"os meus vizinhos ouviram sobre esta ideia.\")\n",
    "print (\"Real translation: and my neighboring homes heard about this idea .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints_transformer/transformer.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints_transformer/transformer.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: vou ento muito rapidamente partilhar convosco algumas histrias de algumas coisas mgicas que aconteceram.\n",
      "Predicted translation: so i 'm going to share very quickly share with you some magic stories that happened there .\n",
      "Real translation: so i 'll just share with you some stories very quickly of some magical things that have happened .\n"
     ]
    }
   ],
   "source": [
    "translate(\"vou ento muito rapidamente partilhar convosco algumas histrias de algumas coisas mgicas que aconteceram.\")\n",
    "print (\"Real translation: so i 'll just share with you some stories very quickly of some magical things that have happened .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Encoder/embedding/embeddings:0' shape=(8216, 128) dtype=float32>\n",
      "<tf.Variable 'Encoder/Encoder_layer_1/mha/fully_connected/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_1/mha/fully_connected/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_1/mha/fully_connected_1/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_1/mha/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_1/mha/fully_connected_2/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_1/mha/fully_connected_2/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_1/mha/fully_connected_3/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_1/mha/fully_connected_3/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_1/layer_normalization/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Encoder/Encoder_layer_1/layer_normalization/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Encoder/Encoder_layer_1/pwffn/fully_connected/weights:0' shape=(128, 512) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_1/pwffn/fully_connected/biases:0' shape=(512,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_1/pwffn/fully_connected_1/weights:0' shape=(512, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_1/pwffn/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_1/layer_normalization_1/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Encoder/Encoder_layer_1/layer_normalization_1/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Encoder/Encoder_layer_2/mha/fully_connected/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_2/mha/fully_connected/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_2/mha/fully_connected_1/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_2/mha/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_2/mha/fully_connected_2/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_2/mha/fully_connected_2/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_2/mha/fully_connected_3/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_2/mha/fully_connected_3/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_2/layer_normalization_2/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Encoder/Encoder_layer_2/layer_normalization_2/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Encoder/Encoder_layer_2/pwffn/fully_connected/weights:0' shape=(128, 512) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_2/pwffn/fully_connected/biases:0' shape=(512,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_2/pwffn/fully_connected_1/weights:0' shape=(512, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_2/pwffn/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_2/layer_normalization_3/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Encoder/Encoder_layer_2/layer_normalization_3/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Encoder/Encoder_layer_3/mha/fully_connected/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_3/mha/fully_connected/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_3/mha/fully_connected_1/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_3/mha/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_3/mha/fully_connected_2/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_3/mha/fully_connected_2/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_3/mha/fully_connected_3/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_3/mha/fully_connected_3/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_3/layer_normalization_4/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Encoder/Encoder_layer_3/layer_normalization_4/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Encoder/Encoder_layer_3/pwffn/fully_connected/weights:0' shape=(128, 512) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_3/pwffn/fully_connected/biases:0' shape=(512,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_3/pwffn/fully_connected_1/weights:0' shape=(512, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_3/pwffn/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_3/layer_normalization_5/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Encoder/Encoder_layer_3/layer_normalization_5/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Encoder/Encoder_layer_4/mha/fully_connected/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_4/mha/fully_connected/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_4/mha/fully_connected_1/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_4/mha/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_4/mha/fully_connected_2/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_4/mha/fully_connected_2/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_4/mha/fully_connected_3/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_4/mha/fully_connected_3/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_4/layer_normalization_6/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Encoder/Encoder_layer_4/layer_normalization_6/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Encoder/Encoder_layer_4/pwffn/fully_connected/weights:0' shape=(128, 512) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_4/pwffn/fully_connected/biases:0' shape=(512,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_4/pwffn/fully_connected_1/weights:0' shape=(512, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_4/pwffn/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/Encoder_layer_4/layer_normalization_7/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Encoder/Encoder_layer_4/layer_normalization_7/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/embedding_1/embeddings:0' shape=(8089, 128) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/mha_1/fully_connected/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/mha_1/fully_connected/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/mha_1/fully_connected_1/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/mha_1/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/mha_1/fully_connected_2/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/mha_1/fully_connected_2/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/mha_1/fully_connected_3/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/mha_1/fully_connected_3/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/layer_normalization_8/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/layer_normalization_8/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/mha_2/fully_connected/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/mha_2/fully_connected/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/mha_2/fully_connected_1/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/mha_2/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/mha_2/fully_connected_2/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/mha_2/fully_connected_2/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/mha_2/fully_connected_3/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/mha_2/fully_connected_3/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/layer_normalization_9/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/layer_normalization_9/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/pwffn/fully_connected/weights:0' shape=(128, 512) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/pwffn/fully_connected/biases:0' shape=(512,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/pwffn/fully_connected_1/weights:0' shape=(512, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/pwffn/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/layer_normalization_10/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_1/layer_normalization_10/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/mha_1/fully_connected/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/mha_1/fully_connected/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/mha_1/fully_connected_1/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/mha_1/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/mha_1/fully_connected_2/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/mha_1/fully_connected_2/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/mha_1/fully_connected_3/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/mha_1/fully_connected_3/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/layer_normalization_11/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/layer_normalization_11/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/mha_2/fully_connected/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/mha_2/fully_connected/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/mha_2/fully_connected_1/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/mha_2/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/mha_2/fully_connected_2/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/mha_2/fully_connected_2/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/mha_2/fully_connected_3/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/mha_2/fully_connected_3/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/layer_normalization_12/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/layer_normalization_12/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/pwffn/fully_connected/weights:0' shape=(128, 512) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/pwffn/fully_connected/biases:0' shape=(512,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/pwffn/fully_connected_1/weights:0' shape=(512, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/pwffn/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/layer_normalization_13/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_2/layer_normalization_13/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/mha_1/fully_connected/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/mha_1/fully_connected/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/mha_1/fully_connected_1/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/mha_1/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/mha_1/fully_connected_2/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/mha_1/fully_connected_2/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/mha_1/fully_connected_3/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/mha_1/fully_connected_3/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/layer_normalization_14/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/layer_normalization_14/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/mha_2/fully_connected/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/mha_2/fully_connected/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/mha_2/fully_connected_1/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/mha_2/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/mha_2/fully_connected_2/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/mha_2/fully_connected_2/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/mha_2/fully_connected_3/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/mha_2/fully_connected_3/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/layer_normalization_15/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/layer_normalization_15/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/pwffn/fully_connected/weights:0' shape=(128, 512) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/pwffn/fully_connected/biases:0' shape=(512,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/pwffn/fully_connected_1/weights:0' shape=(512, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/pwffn/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/layer_normalization_16/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_3/layer_normalization_16/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/mha_1/fully_connected/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/mha_1/fully_connected/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/mha_1/fully_connected_1/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/mha_1/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/mha_1/fully_connected_2/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/mha_1/fully_connected_2/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/mha_1/fully_connected_3/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/mha_1/fully_connected_3/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/layer_normalization_17/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/layer_normalization_17/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/mha_2/fully_connected/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/mha_2/fully_connected/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/mha_2/fully_connected_1/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/mha_2/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/mha_2/fully_connected_2/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/mha_2/fully_connected_2/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/mha_2/fully_connected_3/weights:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/mha_2/fully_connected_3/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/layer_normalization_18/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/layer_normalization_18/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/pwffn/fully_connected/weights:0' shape=(128, 512) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/pwffn/fully_connected/biases:0' shape=(512,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/pwffn/fully_connected_1/weights:0' shape=(512, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/pwffn/fully_connected_1/biases:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/layer_normalization_19/gamma:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'Decoder/Decoder_layer_4/layer_normalization_19/beta:0' shape=(128,) dtype=float32>\n",
      "<tf.Variable 'fully_connected/weights:0' shape=(128, 8089) dtype=float32_ref>\n",
      "<tf.Variable 'fully_connected/biases:0' shape=(8089,) dtype=float32_ref>\n",
      "total number of trainable parameters:  4981913\n"
     ]
    }
   ],
   "source": [
    "model = transformer_model(num_layers = num_layers, d_model = d_model, num_heads = num_heads, d_ff = d_ff, \n",
    "                          input_vocab_size = input_vocab_size, target_vocab_size = target_vocab_size, \n",
    "                          pe_input = input_vocab_size, pe_target = target_vocab_size, rate = dropout_rate)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    total_parameters = 0\n",
    "    for variable in tf.trainable_variables():\n",
    "        print(variable)\n",
    "        # shape is an array of tf.Dimension\n",
    "        shape = variable.get_shape()\n",
    "        variable_parameters = 1\n",
    "        for dim in shape:\n",
    "            variable_parameters *= dim.value\n",
    "\n",
    "        total_parameters += variable_parameters\n",
    "    print('total number of trainable parameters: ',total_parameters)"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "n16CQ",
   "launcher_item_id": "npjGi"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
